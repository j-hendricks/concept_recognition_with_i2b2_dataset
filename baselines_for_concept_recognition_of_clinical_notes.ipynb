{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01b3gHYeRejY",
        "outputId": "c861cb2f-a32f-46f1-f8b3-1df1bf0f2612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hmmlearn==0.2.6\n",
            "  Downloading hmmlearn-0.2.6.tar.gz (155 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.2/155.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.10/dist-packages (from hmmlearn==0.2.6) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.16 in /usr/local/lib/python3.10/dist-packages (from hmmlearn==0.2.6) (1.2.2)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from hmmlearn==0.2.6) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.16->hmmlearn==0.2.6) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.16->hmmlearn==0.2.6) (3.3.0)\n",
            "Building wheels for collected packages: hmmlearn\n",
            "  Building wheel for hmmlearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hmmlearn: filename=hmmlearn-0.2.6-cp310-cp310-linux_x86_64.whl size=464581 sha256=ff4cbe1eed7c30eeca9176617fa7b20eb015cde748eb4f6f0e7b85c8938bfd00\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/9c/0d/ad94b4e1c2388b051cf78a0207f033b08b2c7d15ede782b431\n",
            "Successfully built hmmlearn\n",
            "Installing collected packages: hmmlearn\n",
            "Successfully installed hmmlearn-0.2.6\n"
          ]
        }
      ],
      "source": [
        "# install hmmlearn library for training Hidden Markhov Model\n",
        "!pip install hmmlearn==0.2.6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install crfsuite library for training Conditional Random Fields Classifier\n",
        "!pip install sklearn-crfsuite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_74RjuURzTI",
        "outputId": "6f5316d1-3759-42f1-f71f-f0892fb11521"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn-crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite)\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite) (4.66.2)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.10 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from itertools import chain\n",
        "\n",
        "import nltk\n",
        "import sklearn\n",
        "import scipy.stats\n",
        "\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers, CRF\n",
        "from sklearn_crfsuite import metrics\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from typing import List\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, \\\n",
        "    f1_score, roc_auc_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeJYHnpXRzZY",
        "outputId": "ed7b3f11-dd75-4876-9820-7bf2b99394b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this notebook was run on Google Colab, so data must be pulled from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6Bz8GQbSgCx",
        "outputId": "f6be6ca4-ba31-42ce-dc45-0b549083321b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# file paths to data on personal Google Drive\n",
        "suffix = 'drive/MyDrive/medical_dataset_analysis/'\n",
        "beth_file_directory = 'concept_assertion_relation_training_data/beth/txt/'\n",
        "beth_concept_directory = 'concept_assertion_relation_training_data/beth/concept/'\n",
        "partner_file_directory = 'concept_assertion_relation_training_data/partners/txt/'\n",
        "partner_concept_directory = 'concept_assertion_relation_training_data/partners/concept/'"
      ],
      "metadata": {
        "id": "95l_EiN5SgG6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function that loads all of the the text and annotations from the folders\n",
        "def getNotes(file_directory):\n",
        "    text_dict = {}\n",
        "    file_list = os.listdir(suffix+file_directory)\n",
        "    for f in file_list:\n",
        "        if f[-3:] != 'xml' and f[-3:] != 'txt' and f[-3:] != 'con':\n",
        "            file_list.remove(f)\n",
        "    if '.DS_Store' in file_list:\n",
        "        file_list.remove('.DS_Store')\n",
        "    for file_name in file_list:\n",
        "        with open(suffix+file_directory + file_name,'r') as file:\n",
        "            data = file.read()\n",
        "            text_dict[file_name[:-4]] = data\n",
        "        file.close()\n",
        "    return text_dict"
      ],
      "metadata": {
        "id": "I1ZZr9bjSgKk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beth_notes = getNotes(beth_file_directory)\n",
        "beth_concepts = getNotes(beth_concept_directory)\n",
        "partners_notes = getNotes(partner_file_directory)\n",
        "partners_concepts = getNotes(partner_concept_directory)\n",
        "\n",
        "all_notes = beth_notes.copy()\n",
        "all_notes.update(partners_notes)\n",
        "all_concepts = beth_concepts.copy()\n",
        "all_concepts.update(partners_concepts)"
      ],
      "metadata": {
        "id": "6eg5pVLmSgON"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# need to manually align the text with the annotations and then store in a dictionary\n",
        "start_tag = 'B'\n",
        "inner_tag = 'I'\n",
        "null_tag = 'O'\n",
        "\n",
        "data = {}\n",
        "\n",
        "for record in all_concepts.keys():\n",
        "    data[record] = {'words':[],'tags':[]}\n",
        "    ns = all_notes[record]\n",
        "    cs = all_concepts[record]\n",
        "    lines = ns.split('\\n')\n",
        "    concept_lines = cs.split('\\n')[:-1]\n",
        "\n",
        "    lines_with_concepts = {}\n",
        "\n",
        "    for concept_index, concept_line in enumerate(concept_lines):\n",
        "        note_line = re.findall('[0-9]{1,3}:',concept_line)[0][:-1]\n",
        "        word_nums = list(map(lambda x: int(x[1:]),re.findall(':[0-9]{1,3}',concept_line)))\n",
        "        lines_with_concepts[(note_line,str(concept_index))] = word_nums\n",
        "\n",
        "    for tup, word_nums in lines_with_concepts.items():\n",
        "\n",
        "        insert_dict = {}\n",
        "\n",
        "        notes_index, concepts_index = tup\n",
        "\n",
        "        the_words = lines[int(notes_index)-1].split(' ')\n",
        "        tags = [null_tag]*len(the_words)\n",
        "        for word_num in range(word_nums[0],word_nums[-1]+1):\n",
        "            tag_type = re.findall(r't=\\\"[a-z]+\\\"',concept_lines[int(concepts_index)])[0][3:-1]\n",
        "            if word_num == min(word_nums) and len(word_nums)<3:\n",
        "                tags[word_num] = start_tag + '-' + tag_type\n",
        "            else:\n",
        "                if len(word_nums) < 3:\n",
        "                    tags[word_num] = inner_tag + '-' + tag_type\n",
        "        data[record]['words'] += the_words\n",
        "        data[record]['tags'] +=  tags"
      ],
      "metadata": {
        "id": "y5ZCRrmgSm-r"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store dictionary in dataframe\n",
        "# have to transpose the dataframe\n",
        "df = pd.DataFrame(data)\n",
        "df_trans = df.transpose()"
      ],
      "metadata": {
        "id": "uHO4rOVTSnYD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reorganizing the data so that it matches the format needed for HMM code\n",
        "all_records = []\n",
        "all_words = []\n",
        "all_tags = []\n",
        "for record in df_trans.index:\n",
        "  data = df_trans.loc[record]\n",
        "  words = data['words']\n",
        "  tags = data['tags']\n",
        "  pairs = zip(words,tags)\n",
        "  for word, tag in pairs:\n",
        "    all_records.append(record)\n",
        "    all_words.append(word)\n",
        "    all_tags.append(tag)"
      ],
      "metadata": {
        "id": "pqJguNx4SnqB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new_df is the dataframe containing all the data in the required format\n",
        "new_df = pd.DataFrame({'records':all_records,'words':all_words,'tags':all_tags})\n",
        "new_df.head()"
      ],
      "metadata": {
        "id": "UgslVNXBSn5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hidden Markhov Model**"
      ],
      "metadata": {
        "id": "Lg4y7cxqax5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the next 10 cells are directly sourced from a kaggle tutorial: https://www.kaggle.com/code/annsanababy/hidden-markov-model-hmm-on-ner-dataset\n",
        "X = new_df.drop(columns= ['tags'], axis=1)\n",
        "y = new_df.drop(columns= ['words'], axis=1)"
      ],
      "metadata": {
        "id": "XvvKpe0ITFdn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GroupShuffleSplit is used so that the records are randomized rather than each row in the dataframe being randomized\n",
        "gs = GroupShuffleSplit(n_splits=2, test_size=.1, random_state=42)\n",
        "train_ix, test_ix = next(gs.split(X, y, groups=new_df['records']))\n",
        "\n",
        "data_train = new_df.loc[train_ix]\n",
        "data_test = new_df.loc[test_ix]"
      ],
      "metadata": {
        "id": "-UPppLkVTFz3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# basic preprocessing. Note that more preprocessing could be performed, e.g. removing redundant formatting in the discharge notes\n",
        "def pre_processing(text_column):\n",
        "    # lowercase all text in the column\n",
        "    text_column = text_column.str.lower()\n",
        "\n",
        "    # replacing numbers with NUM token\n",
        "    text_column = text_column.str.replace(r'\\d+', 'NUM')\n",
        "\n",
        "    # removing stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text_column = text_column.apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "\n",
        "    return text_column"
      ],
      "metadata": {
        "id": "2M78WnTVTGOI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Markhov models calculate conditional probabilities according to Bayes' Theorem, which can lead to undefined probabilities since never-before-seen words will have a probability of zero. Using an \"UNKNOWN\" tag prevents this\n",
        "dfupdate = data_train.sample(frac=.15, replace=False, random_state=42)\n",
        "dfupdate.words = 'UNKNOWN'\n",
        "data_train.update(dfupdate)\n",
        "tags = list(set(data_train.tags.values))\n",
        "words = list(set(data_train.words.values))\n",
        "# Convert words and tags into numbers\n",
        "word2id = {w: i for i, w in enumerate(words)}\n",
        "tag2id = {t: i for i, t in enumerate(tags)}\n",
        "id2tag = {i: t for i, t in enumerate(tags)}\n",
        "len(tags), len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxg-shnUTGny",
        "outputId": "75421fe9-62d7-4e05-ea33-d5c2c5ee3747"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 10615)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we construct our transition matrix by first counting the frequency of words\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "count_tags = dict(data_train.tags.value_counts())  # Total number of POS tags in the dataset\n",
        "# Now let's create the tags to words count\n",
        "count_tags_to_words = data_train.groupby(['tags']).apply(\n",
        "    lambda grp: grp.groupby('words')['tags'].count().to_dict()).to_dict()\n",
        "# We shall also collect the counts for the first tags in the sentence\n",
        "count_init_tags = dict(data_train.groupby('records').first().tags.value_counts())\n",
        "\n",
        "# Create a mapping that stores the frequency of transitions in tags to it's next tags\n",
        "count_tags_to_next_tags = np.zeros((len(tags), len(tags)), dtype=int)\n",
        "sentences = list(data_train.records)\n",
        "pos = list(data_train.tags)\n",
        "for i in tqdm(range(len(sentences)), position=0, leave=True):\n",
        "    if (i > 0) and (sentences[i] == sentences[i - 1]):\n",
        "        prevtagid = tag2id[pos[i - 1]]\n",
        "        nexttagid = tag2id[pos[i]]\n",
        "        count_tags_to_next_tags[prevtagid][nexttagid] += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKR634FZTqXI",
        "outputId": "4318a499-2613-44e1-9b16-297e00a75e34"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 304358/304358 [00:00<00:00, 685373.58it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we calculate the transition probabilities\n",
        "startprob = np.zeros((len(tags),))\n",
        "transmat = np.zeros((len(tags), len(tags)))\n",
        "emissionprob = np.zeros((len(tags), len(words)))\n",
        "num_sentences = sum(count_init_tags.values())\n",
        "sum_tags_to_next_tags = np.sum(count_tags_to_next_tags, axis=1)\n",
        "for tag, tagid in tqdm(tag2id.items(), position=0, leave=True):\n",
        "    floatCountTag = float(count_tags.get(tag, 0))\n",
        "    startprob[tagid] = count_init_tags.get(tag, 0) / num_sentences\n",
        "    for word, wordid in word2id.items():\n",
        "        emissionprob[tagid][wordid] = count_tags_to_words.get(tag, {}).get(word, 0) / floatCountTag\n",
        "    for tag2, tagid2 in tag2id.items():\n",
        "        transmat[tagid][tagid2] = count_tags_to_next_tags[tagid][tagid2] / sum_tags_to_next_tags[tagid]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTq6sorvTqyC",
        "outputId": "58ee9899-33d5-4c76-c9ac-83b672dbf5ea"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [00:00<00:00, 24.13it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to create word transition matrix\n",
        "\n",
        "#first step is to count the number of times each word appears in the dataset\n",
        "count_words = {}\n",
        "for word in data_train.words.values:\n",
        "    count_words[word] = count_words.get(word, 0) + 1\n",
        "\n",
        "# then count the number of times a word appears after another word\n",
        "count_word_transitions = {}\n",
        "for sentence in data_train.groupby('records'):\n",
        "    words = sentence[1]['words'].values\n",
        "    for i in range(len(words) - 1):\n",
        "        w1, w2 = words[i], words[i+1]\n",
        "        if w1 not in count_word_transitions:\n",
        "            count_word_transitions[w1] = {}\n",
        "        count_word_transitions[w1][w2] = count_word_transitions[w1].get(w2, 0) + 1\n",
        "\n",
        "# convert the counts to probabilities\n",
        "word_transition_matrix = np.zeros((len(word2id)+1, len(word2id)+1))\n",
        "sum_words_to_next_words = np.sum([count_word_transitions[w1][w2] for w1 in count_word_transitions for w2 in count_word_transitions[w1]])\n",
        "for w1, w1id in word2id.items():\n",
        "    for w2, w2id in word2id.items():\n",
        "        word_transition_matrix[w1id][w2id] = count_word_transitions.get(w1, {}).get(w2, 0) / sum_words_to_next_words\n",
        "print(word_transition_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY2m63MxTrKM",
        "outputId": "0163214c-8f6e-4c4c-92ed-23ab5dc652b5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10616, 10616)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# label new words as \"UNKNOWN\" to avoid an undefined probability\n",
        "data_test.loc[~data_test['words'].isin(words), 'words'] = 'UNKNOWN'\n",
        "word_test = list(data_test.words)\n",
        "samples = []\n",
        "for i, val in enumerate(word_test):\n",
        "    samples.append([word2id[val]])\n",
        "\n",
        "lengths = []\n",
        "count = 0\n",
        "sentences = list(data_test.records)\n",
        "for i in tqdm(range(len(sentences)), position=0, leave=True):\n",
        "    if (i > 0) and (sentences[i] == sentences[i - 1]):\n",
        "        count += 1\n",
        "    elif i > 0:\n",
        "        lengths.append(count)\n",
        "        count = 1\n",
        "    else:\n",
        "        count = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CyfuZWlTrj3",
        "outputId": "4c96bf54-3988-4632-914d-c7ee7515e9cf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 33872/33872 [00:00<00:00, 778505.48it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we use the Viterbi algorithm to efficiently find the most probable sequence of concepts from the transition matrix\n",
        "\n",
        "import hmmlearn\n",
        "from hmmlearn import hmm\n",
        "\n",
        "model = hmm.MultinomialHMM(n_components=len(tags), algorithm='viterbi')\n",
        "model.startprob_ = startprob\n",
        "model.transmat_ = transmat\n",
        "model.emissionprob_ = emissionprob"
      ],
      "metadata": {
        "id": "-kPzjmruTr8N"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zRBhxo9TZm75"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concept_predict = model.predict(samples, lengths)\n",
        "concept_predict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxX0zQZ8T0vB",
        "outputId": "963030fa-bb50-473d-fe95-eae0816cbf32"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 4, 4, ..., 4, 4, 4], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_actual = data_test['tags'].apply(lambda x: tag2id[x])"
      ],
      "metadata": {
        "id": "8xJyeC5QT1FQ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification_report_hmm = classification_report(pos_actual[:len(concept_predict)],concept_predict)\n",
        "print(\"--Results for HMM--\")\n",
        "print(classification_report_hmm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0UKUHodT1Zl",
        "outputId": "7cd56738-f1b2-47b1-ac3e-6dd255109afa"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--Results for HMM--\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.03      0.05       469\n",
            "           1       0.28      0.05      0.08      1044\n",
            "           2       0.23      0.02      0.04       729\n",
            "           3       0.19      0.01      0.02       343\n",
            "           4       0.90      0.99      0.94     29496\n",
            "           5       0.30      0.01      0.01       487\n",
            "           6       0.44      0.02      0.03       437\n",
            "\n",
            "    accuracy                           0.89     33005\n",
            "   macro avg       0.39      0.16      0.17     33005\n",
            "weighted avg       0.83      0.89      0.85     33005\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conditional Random Fields**"
      ],
      "metadata": {
        "id": "NfUkJ_oTaohL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the next 5 cells are directly sourced from a kaggle tutorial: https://www.kaggle.com/code/bavalpreet26/ner-using-crf\n",
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[-3:]': word[-3:],\n",
        "        'word[-2:]': word[-2:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "Llj2ek_5UOUk"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, label in sent]"
      ],
      "metadata": {
        "id": "Z3yPdku0UOro"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = list()\n",
        "for record in df_trans.index:\n",
        "  data = df_trans.loc[record]\n",
        "  words = data['words']\n",
        "  tags = data['tags']\n",
        "  pairs = zip(words,tags)\n",
        "  for word, tag in pairs:\n",
        "    sentences.append([(word,tag)])"
      ],
      "metadata": {
        "id": "8p5VpTNeUPBU"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = [sent2features(s) for s in sentences]\n",
        "y = [sent2labels(s) for s in sentences]"
      ],
      "metadata": {
        "id": "s2E0Nj_gUPXB"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crf = CRF(algorithm='lbfgs',\n",
        "          c1=0.1,\n",
        "          c2=0.1,\n",
        "          max_iterations=100,\n",
        "          all_possible_transitions=False)"
      ],
      "metadata": {
        "id": "AzEDVtecUPsb"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I placed a try and except block because the original code would throw an AttributeError due to a field in the crf object that has no consequence to my model.\n",
        "# I used the first 12000 tokens has my train set and the remaining 5000 as my test set\n",
        "try:\n",
        "  crf.fit(X[:12000],y[:12000])\n",
        "except AttributeError:\n",
        "  pass\n",
        "\n",
        "pred = crf.predict(X[12000:])\n",
        "\n",
        "classification_report_crf = classification_report(y[12000:],pred)\n",
        "print(\"--Results for CRF--\")\n",
        "print(classification_report_crf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4bIKYBlT1tP",
        "outputId": "049339eb-0742-4866-9179-c79030e73e0a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--Results for CRF--\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   B-problem       0.26      0.05      0.09      6802\n",
            "      B-test       0.18      0.03      0.05      4444\n",
            " B-treatment       0.24      0.12      0.16      4613\n",
            "   I-problem       0.22      0.05      0.09      9900\n",
            "      I-test       0.18      0.02      0.03      3757\n",
            " I-treatment       0.20      0.05      0.08      3907\n",
            "           O       0.90      0.98      0.94    292807\n",
            "\n",
            "    accuracy                           0.89    326230\n",
            "   macro avg       0.31      0.19      0.21    326230\n",
            "weighted avg       0.83      0.89      0.85    326230\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "uNI9pewub_iI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both models achieve F1 scores near 0.2, which is far below the benchmark of 0.7-0.9. For next steps, I will fine-tune a BERT model on the same task. This model should perform closer to the benchmark given it is state-of-the-art."
      ],
      "metadata": {
        "id": "X7vE8NfgcFAE"
      }
    }
  ]
}